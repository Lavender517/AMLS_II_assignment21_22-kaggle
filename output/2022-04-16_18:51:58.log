Using hyperparameters as: Namespace(sen_len=30, fixed_embedding=True, embedding_dim=200, batch_size=64, num_workers=16, n_epochs=20, lr=5e-06, weight_decay=0.05, dropout=0.6, device=5, early_stop_TH=6, model='BERT')
Loading training data ...
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using BERT Model
start training, parameter total:109483009, trainable:109483009

Epoch1: 96/96 
Train | Loss:0.21979 Acc: 67.188
Valid | Loss:0.16090 Acc: 77.539 
-----------------------------------------------

Epoch2: 96/96 
Train | Loss:0.15526 Acc: 78.532
Valid | Loss:0.15253 Acc: 79.557 
-----------------------------------------------

Epoch3: 96/96 
Train | Loss:0.13999 Acc: 80.615
Valid | Loss:0.13844 Acc: 80.599 
-----------------------------------------------

Epoch4: 96/96 
Train | Loss:0.12861 Acc: 82.275
Valid | Loss:0.13493 Acc: 81.250 
saving model with acc 81.250
-----------------------------------------------

Epoch5: 96/96 
Train | Loss:0.12307 Acc: 83.154
Valid | Loss:0.13550 Acc: 81.120 
-----------------------------------------------

Epoch6: 96/96 
Train | Loss:0.11442 Acc: 84.277
Valid | Loss:0.13371 Acc: 81.250 
-----------------------------------------------

Epoch7: 96/96 
Train | Loss:0.10842 Acc: 85.596
Valid | Loss:0.14078 Acc: 81.380 
saving model with acc 81.380
-----------------------------------------------

Epoch8: 96/96 
Train | Loss:0.10300 Acc: 86.377
Valid | Loss:0.14301 Acc: 81.250 
-----------------------------------------------

Epoch9: 96/96 
Train | Loss:0.09616 Acc: 87.207
Valid | Loss:0.13962 Acc: 81.510 
Traceback (most recent call last):
  File "/home/uceehx2/AMLS_II_assignment21_22-kaggle/main.py", line 131, in <module>
    train()
  File "/home/uceehx2/AMLS_II_assignment21_22-kaggle/main.py", line 100, in train
    training(args.model, args.batch_size, args.n_epochs, criterion, optimizer, scheduler, train_loader, val_loader, train_model, device, args.early_stop_TH)
  File "/home/uceehx2/AMLS_II_assignment21_22-kaggle/code/train_test.py", line 92, in training
    torch.save(model, "./models/" + modeltype + "/ckpt_" + str(round(total_acc_new, 3)) + ".model")
  File "/home/uceehx2/AMLS/lib/python3.9/site-packages/torch/serialization.py", line 373, in save
    return
  File "/home/uceehx2/AMLS/lib/python3.9/site-packages/torch/serialization.py", line 214, in __exit__
    self.file_like.close()
OSError: [Errno 122] Disk quota exceeded
